---
title: "Workshop Datenanalyse"
subtitle: "mit einem Schwerpunkt auf Versicherungsunternehmen"
author: "Dr.-Ing. Andreas Cardeneo"
institute: "Data Scientist bei Allianz Lebensversicherungs-AG"
email: "andreas.cardeneo@lehre.dhbw-stuttgart.de"
date: "2020-01-29 (commit ID `r substr(system('git rev-parse HEAD', intern = TRUE), 1, 7)`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"

--- 
class: inverse, center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

library(emo)
library(kableExtra)
library(ggalluvial)
library(tidyverse)
library(easyalluvial)
```

# Datenanalyse in Versicherungsunternehmen?

Wir sammeln mal ein paar Aufgaben, die dazu passen.


---
# Der Prozess der Datenanalyse

1. Die richtigen Fragen finden: Was möchte ich eigentlich wissen?
2. Die eigentliche Datenanalyse:

   ![Datenanalyseprozess](./img/data-science-explore.png)

Quelle: [*R for Data Science*](https://r4ds.had.co.nz/explore-intro.html)

Der Punkt *Model* in obiger Graphik bezieht sich auf Datenanalyse im Kontext von *Data Science*, wenn es darum geht, aus den Daten deskriptive oder prädiktive Modelle abzuleiten.

Auf den folgenden Seiten werden die einzelnen Schritte etwas genauer erklärt.


---
# Daten extrahieren

* *Datenextrakt*: Beziehen der Daten aus den verschiedenen Datenquellen.

`r emo::ji("question")` Woher beziehen wir eigentlich Daten?


---
# Datenquellen in VU (1)

* Datenbanken - strukturierte Daten
  * OLTP<sup>1</sup>-Systeme z.B. für die Vertragsverwaltung, Zahlvorgänge, Finanzbuchungen
  * OLAP<sup>2</sup>-Systeme für Reporting (Berichtswesen): Nilden die technische Grundlage für  BI<sup>3</sup>-Systeme
  
Meistens werden *relationale Datenbanken* verwendet, auf die mittels `SQL` zugegriffen wird.

.footnote[
[1] *OLTP* - Online Transaction Processing, IT-Systeme, die eine hohe Zahl vergleichsweise kleiner Datenverarbeitungsschritte ausführen, z.B. Datensatzänderungen, Neuanlage von Datensätzen.

[2] *OLAP* - Online Analytical Processing, (interaktive) IT-Systeme für Analysen auf größeren Datenmengen.

[3] *Business Intelligence* - betriebliche Informationssysteme mit Fokus auf vergangenheitsorientiertes Berichtswesen.
]

---
# Datenquellen in VU (2)

* Verwaltungssysteme für unstrukturierte Daten
  * Dokumentenarchive bspw. für Briefe von/an Kunden, Rechtsanwälte, Gutachter, Vertreter usw.
  * Archive für Bilddaten, bspw. Dokumentation von Schadensfällen
  
Diese Verwaltungssysteme lassen sich üblicherweise nicht in vergleichsweise standardisierter Form abfragen wie relationale Datenbanken und die Inhalte müssen erst weiter verarbeitet werden, bspw. mittels OCR<sup>1</sup>. 

.footnote[
[1] *OCR* - Optical Character Recognition, maschinelle Erkennung von (Schrift-)Zeichen, zumeist inkl. Erkennung des Kontexts (Anschrift, Anrede, Betreff, Brieftext usw.).
]

---
# Datenquellen in VU (3)

* Anwendungsdaten
    * insbesondere Office
         * insbesondere `Excel`
  
  Hier liegen die Daten meistens in Form von Dateien in einem Verzeichnis vor. Möchte man die Daten analysieren, müssen diese erst aus den Dokumenten extrahiert oder aus den Anwendungen heraus exportiert werden. Typischer Fall: `CSV`-Dateien für tabellarische Daten.
  
* Technische Protokolle

  Die meisten *Backend*-Systeme wie bspw. *Webserver* protokollieren Zugriffe in Protokolldateien (*Log*-Dateien). Diese Dateien werden normalerweise rollierend überschrieben, bieten aber auf technischer Ebene detaillierte Informationen und insbesondere Zeitstempel. 

  Die Protokolle werden in ähnlicher oder gleicher Form oftmals auch in Datenbanktabellen geschrieben.
        
---
# Datenquellen in VU (4)

* APIs<sup>1</sup>
  
  Modernere IT-Architekturen (heute vielfach [Microservice-Architekturen](https://www.ionos.de/digitalguide/websites/web-entwicklung/microservice-architecture-so-funktionieren-microservices/) definieren technische Schnittstellen, damit die anbietenden Systeme auf relative einfache Weise von außen angesteuert werden können. APIs ermöglichen insbesondere die Abfrage von Zuständen und Daten. Beispiele sind Finanzdaten, Wetterdaten. 

**Beispiel:** Anbieter *Quandl* liefert u.a. Finanzdaten: [https://www.quandl.com/api/v3/datasets/FSE/ALV_X.json?api_key=XXXXXX](https://www.quandl.com/api/v3/datasets/FSE/ALV_X.json)

Für alle oben genannten Datenquellen gilt: Die meisten davon werden überall in Unternehmen genutzt und sind nicht spezifisch für die Versicherungsbranche.

**Was könnte für die Versicherungsbranche besonders sein?**


.footnote[
*API* - Application Programming Interface, technische Schnittstelle zu einem IT-System, um dieses durch andere Programme zu nutzen, d.h. durch die API wird ein IT-System maschinell nutzbar.
]

---
# Besonderheiten von Datenquellen in VU

Versicherungen sind oftmals sehr langlebige Produkte:
* Verträge (bspw. für Lebensversicherungen) laufen über mehrere Jahrzehnte

    `r emo::ji("question")` Wie haben sich IT-Systeme in der Zeit geändert?
    
  &rarr; Wir haben es oftmals mit sehr verschiedenen Technologien zu tun, die *integriert* werden müssen.
  
  &rarr; Betrifft nicht nur Hardware, sondern auch Software z.B. Umlaute in alten Datenbeständen (7 Bit ASCII, kein UTF).
  
  &rarr; Datenstrukturen (bspw. Tabellenschemata) verändern sich über die Zeit wg. wandelnder Anforderungen.


---
# Datenanalyse per Code 

* In vielen Unternehmen wird für die Datenanalyse GUI-basierte Software wie bspw. *PowerBI*, *Tableau*, *Microstrategy*,...<sup>1</sup> verwendet.
* Wird die Datenanalyse per *Code* ausgeführt, also programmmiert, gibt es folgende Vorteile:
    * die *Reproduzierbarkeit* der Ergebnisse ist besser
    * erleichterte *Kollaboration* mittels Versionsverwaltung 
    * weit größere Möglichkeiten der Datentransformation durch *Programmierbarkeit*
    * [Artikel zu Data Science und Programmierung](https://blog.dominodatalab.com/data-scientist-programmer-mutually-exclusive/)


`r emo::ji("backhand index pointing right")` [Crashkurs *Programmieren in R*](https://datentaeter.de/crashkurs-programmieren-in-r-fuer-journalismus-rstudio-coden-datenjournalismus-installation-tutorial/) 

`r emo::ji("backhand index pointing right")` [Datenanalyse in R  1](http://christopherharms.de/stuff/r-workshop/R-Workshop_UniBonn_Tag1.pdf)

`r emo::ji("backhand index pointing right")` [Datenanalyse in R  2](http://christopherharms.de/stuff/r-workshop/R-Workshop_UniBonn_Tag2.pdf)


    
.footnote[
[1] Einige dieser Anwendungen sind auch in gewissem Umfang programmierbar.
]

---
# Datenschutz
	
* *Datenschutzgrundverordnung* (DGSVO) seit 25. Mai 2018 wirksam
* Schutz personenbezogener Daten in der Europäischen Union

`r emo::ji("question")` Was sind *personenbezogene Daten*?

Nach [Art. 4 (1) der DSGVO](https://dejure.org/gesetze/DSGVO/4.html)
> personenbezogene Daten [sind] alle Informationen, die sich auf eine identifizierte oder identifizierbare 
> natürliche Person (im Folgenden "betroffene Person") beziehen; als identifizierbar wird eine natürliche
> Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem
> Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung oder zu einem oder mehreren
> besonderen Merkmalen, die Ausdruck der physischen, physiologischen, genetischen, psychischen, 
> wirtschaftlichen, kulturellen oder sozialen Identität dieser natürlichen Person sind, identifiziert werden
> kann.

---
# Umgang mit personenbezogenen Daten

`r emo::ji("question")` Enthalten die Datenquellen personenbezogene Daten?

`r emo::ji("question")` Werden für die Datenanalyse wirklich personenbezogene Daten (*pbDaten*) benötigt?

* Daten können *anonymisiert* oder *pseudonymisiert* werden:
    * **Anonymisierung**: pbDaten werden unumkehrbar verändert, eine Zuordnung von Merkmalsausprägungen zu Personen ist nicht mehr herstellbar
    * **Pseudonymisierung**: Erklärung nach [Art. 4 DSGVO](https://dsgvo-gesetz.de/art-4-dsgvo/):
    > die Verarbeitung personenbezogener Daten in einer Weise, dass die personenbezogenen Daten ohne Hinzuziehung zusätzlicher Informationen nicht mehr einer spezifischen betroffenen Person zugeordnet werden können, sofern diese zusätzlichen Informationen gesondert aufbewahrt werden und technischen und organisatorischen Maßnahmen unterliegen, die gewährleisten, dass die personenbezogenen Daten nicht einer identifizierten oder identifizierbaren natürlichen Person zugewiesen werden;

---
# Typische Transformationen personenbezogener Daten

* Entfernen von Namen (für Analysezwecke werden diese typischerweise nicht benötigt)
* Geburtsdatum durch Alter ersetzen
* Telefonnummern verkürzen (Vorwahl ausreichend?)
* IBAN: Auf Bankleitzahl verkürzen
* IP-Adresse: Ohnehin nicht aussagekräftig -- Geolokation stattdessen?
* Adresse verkürzen, bspw. auf Postleitzahl
* Analysen mit regionalem Bezug nicht auf kleine Regionen anwenden

---
# Reproduzierbarkeit der Datenanalyse - Versionierung (1)

Das hier passiert sehr oft in Unternehmen (Bildquelle: [datascience.box](https://github.com/rstudio-education/datascience-box)):

.center[![:scale 40%](./img/phd_comics_vc.gif)]

---
# Reproduzierbarkeit der Datenanalyse - Versionierung (2)

Versionsverwaltungssysteme  (Bildquelle: [datascience.box](https://github.com/rstudio-education/datascience-box)) helfen bei der sicheren Verwaltung von Änderungen: * Nachverfolgen der Änderungen (was und durch wen) und 
* Verhinderung bzw. Behebung von Konflikten bei Zusammenführungen

.center[![:scale 40%](./img/lego-steps-commit-messages.png)]

---
# Reproduzierbarkeit der Datenanalyse - Versionierung (3)

Ein heute sehr verbreitetes Versionskontrollsystem ist [git](https://www.git-scm.com/). Ein vereinfachter Arbeitsfluss mit *git* sieht so aus (Bildquelle: [Git Workflow blog post](https://nvie.com/posts/a-successful-git-branching-model/)):

.center[![:scale 35%](./img/main-branches.png)]


---
# Reproduzierbarkeit der Datenanalyse - Data lineage

Unter *data lineage* oder *data provenance* versteht man die Fähigkeit, jederzeit den Ursprung, die (fachliche) Bedeutung und die Transformationskette von Daten nachvollziehen zu können.

&rarr; In Verbindung mit *Versionierung* kann man mittels *data lineage* insbesondere Änderungen in der Datenverarbeitungskette und der Datenursprünge über die Zeit verfolgen.

&rarr; *Data lineage* stützt sich idealerweise auf ein System zum *Metadatenmanagement*, d.h. einem Katalog von Daten über Daten, u.a. zu Dateneignern, Feldbedeutungen und Datenfeldverwendungsverweisen.


---
# Reproduzierbarkeit der Datenanalyse - Datenintegrität (1)

Datenquellen werden oftmals an einem Ort zusammengeführt und dabei passieren auch Fehler. Es ist hilfreich im Nachhinein einzelne Datensätze den Ladevorgängen zuordnen zu können. Damit kann man u.a. doppelte Ladevorgänge identifizieren.

&rarr; Beim Extrakt ein zusätzliches Feld mit Quellennamen und Zeitstempel hinzufügen.
```
select FELD1, ..., FELDn, 
'QUELLE' || TO_CHAR(SYSDATE, 'YYYY-MM-DD HH24:MI:SS') as SOURCE_TS  
into ZIELTAB 
from QUELLTAB 
where KRIT
```
---
# Reproduzierbarkeit der Datenanalyse - Datenintegrität (2)

Manchmal möchte man erkennen,
* dass sich Dateien verändert haben oder
* dass sich Dateien  *nicht* verändert haben.

In beiden Fällen kann man einen *Fingerabdruck* der Datei bestimmen: Ein Hashwert der Datei:
```{r}
digest::digest("./telco_dataset.csv", algo = "xxhash32", file = TRUE)
```

Indem man den Hashwert zusammen mit dem Dateinamen speichert, kann man die beiden Fälle oben prüfen und bspw. sicherstellen:
* dass ein Importvorgang *nicht* wiederholt wird oder
* dass eine Analyse auf unveränderten Daten arbeitet.

---
# Reproduzierbarkeit der Datenanalyse (4)

* technischer Schlüssel


---
# Schritt: Datenquellen lesen

* Datenformat (Trennzeichen, Quoting...)
* Encoding 


---
# Umgang mit sehr großen Datenmengen

* Datenumfang manchmal größer als verfügbare Rechneressourcen (Speicher)
* Einlesen von Teilmengen
    * Deterministisches Einlesen eines Teils der Daten
```{r, warnings = FALSE, message = FALSE}
somerows <- readr::read_csv("./telco_dataset.csv", n_max = 1000)
```
    * Zufälliges Einlesen
* Verteilte Verarbeitung
    * Erfordert geeignete technische Infrastruktur
        * Cluster von Rechnern
        * Software für die verteilte Speicherung und Verarbeitung
            * Hadoop (HDFS Dateisystem)
            * Spark, Pig, ...


---
# Schritt: Daten verstehen 

* Variablen und Beobachtungen
* Validieren
* Datentypen
* Plotten 
* QS: Magic Numbers

---
# Die allgemeine Struktur von Daten

In den Daten einer Datenanalyse können wir immer wieder die gleiche Struktur erkennen:

* Die Daten bestehen aus einer Menge von gleichartigen *Datensätzen*, auch *Instanzen* oder *Objekte* genannt,
* wobei die einzelnen Datensätze durch bestimmte *Eigenschaften* oder *Merkmale* oder *Variablen* beschrieben werden 
* und diese Merkmale pro Instanz bestimmte *Ausprägungen* haben, die auch *Beobachtungen* genannt werden<sup>1</sup>.

`r emo::ji("backhand index pointing right")` Unternehmensdaten können meistens in Tabellenform extrahiert werden. Die Zeilen entsprechen dann meistens den Instanzen und die Spalten(namen) den Variablen. Die Werte in den Tabellenzellen entsprechen den Beobachtungen. 

.footnote[
[1] Manchmal wird auch die Gesamtheit der Ausprägungen zu einer Instanz Beobachtung genannt. Man spricht dann auch von einem *Datenpunkt* in einem (mehrdimensionalem) Raum.
]


---
# Typen von Variablen

Man unterscheidet zwischen verschiedenen Typen von Variablen:

.pull-left[
  * *Kategoriell*: Die Ausprägungen der Variablen beschreiben *Kategorien*
  * liegen typischerweise als Text vor, bspw. J/N oder M/W 
  * können aber auch Zahlenwerte sein wie 0/1 und als Wahrheitswerte dienen. 
  * Meistens wenig verschiedene Ausprägungen pro Variable.
  * In `R` werden kategorielle Variablen oftmals als *factor* codiert.
  * Nur sinnvoll auf (Un-)Gleichheit prüfbar, kein Rechnen.
]

.pull-right[
  * *Quantitativ*: Die Ausprägungen sind numerische Werte.
  * *Ordinalen* Ausprägungen kann man sinnvoll sortieren aber sie eignen sich nicht zum Rechnen. **Beispiel**: Rangziffern
  * Mit *kardinalen* Ausprägungen kann man rechnen und bspw. Differenzen bilden ([Intervallskala](https://de.wikipedia.org/wiki/Skalenniveau)) oder Verhältnisse ausrechnen ([Verhältnisskala](https://de.wikipedia.org/wiki/Skalenniveau)).
  * Als quantitative Ausprägungen können auch Zeit- und Datumswerte angesehen werden. 
]



---
# Unstrukturierte Daten
# 

Unter *unstrukturierte Daten* versteht man oft Texte, Bilder, Videos oder Audiodaten. 
  * In technischer Hinsicht sind die Daten strukturiert, weil sie in einem definierten Format vorliegen.
  * Für die Analyse ist man aber eher am *Inhalt* der Daten interessiert. 
      * Dann benötigt man Techniken des NLP<sup>1</sup> oder der [Objekterkennung in Bildern](https://medium.com/alegion/deep-learning-for-object-detection-and-localization-using-r-cnn-e88f85ea7c16)):
      
.center[![:scale 30%](./img/bilderobjekte.png)]
      
.footnote[
[1] *NLP*: Natural Language Processing: Methoden zur Verarbeitung natürlicher Sprache. Ziel ist dabei das Textverständnis, um bspw. die Stimmung in Texten zu erkennen (&rarr; [Sentiment Analysis](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html)).
]

---
# Qualitätssicherung: Ausreißeranalyse



---
# Qualitätssicherung:  Fehlende Werte

Sehr häufig sind Felder in Tabellen nicht befüllt. 

`r emo::ji("question")` Wie sollen diese fehlenden Werte in der Datenanalyse behandelt werden?

* Handelt es sich dabei um einen Fehler? 
    * Ist der Fehler beim Einlesen passiert oder ist er in den Ausgangsdaten zu finden?
* Haben fehlende Werte eine Bedeutung?
    * Sehr oft bedeutet ein fehlender Wert, dass das jeweilige Merkmal für die Instanz nicht vorhanden oder nicht relevant ist.
       **Beispiel**: Feld *letzter Operationstermin* in einem Antrag auf Berufsunfähigkeitsversicherung
* Es gibt verschiedene Strategien, um mit fehlenden Werten umzugehen:
    * Ignorieren
    * Ersetzen
    * Löschen



---
# Qualitätssicherung:  Besondere Werte

Gerade in älteren IT-Systeme (*Legacy*-Systemen) wurden in der Vergangenheit gerne spezielle Werte (Ausprägungen) verwendet, um qualitative Aussagen zu treffen:
* Grund ist oft, dass die Applikation die Eingabe von Werten in der UI verlangt (*Pflichtfelder*).
* Die verwendeten Werte sind für Menschen einfach und offensichtlich als falsch erkennbar und werden korrekt als Dummy-Werte interpretiert.
* Für eine maschinellen Analyse muss diese Interpretation explizit in die Auswertung eingebracht werden.

Man findet häufig:
* 9, 99, 999, ...: Wird oft als Kennzeichen für *irrelevant* oder *fehlt* verwendet, z.B. bei Altersangaben, Telefonnummern usw. 
* "␣" (Leerzeichen) als Kennzeichen für *fehlende* Werte.
* "" (leere Zeichenkette) als Kennzeichen für *fehlende* Werte.


---
# Datenanalyse

* Boxplots, violin plots
* Histogramme
* Scatterplots, 
* Korrelation
* Tests auf statistische Unabhängigkeit
* 

---
 


---
# Visualisierung: Histogramm

Ein *Histogramm* ist ein Häufigkeitsdiagramm. Es dient dazu, die Häufigkeit von Merkmalsausprägungen im Vergleich zu veranschaulichen.

**Beispiel** anhand des `diamonds`-Datensatzes aus `R`:
```{r}
diamonds %>% ggplot() + geom_bar(aes(x = cut))
```


---
# Visualisierung: Boxplot


---
# 

---
# Visualisierung: Alluvial-Diagramme

* ähnlich zu *Parallelkoordinaten* aber für kategorielle Variablen
* `R`-Paket: `ggalluvial`

```{r echo = FALSE}
ggplot(as.data.frame(Titanic),
       aes(y = Freq,
           axis1 = Sex, axis2 = Class, axis3 = Survived)) +
  geom_alluvium(aes(fill = Class),
                width = 0, knot.pos = 0, reverse = FALSE) +
  guides(fill = FALSE) +
  geom_stratum(width = 1/8, reverse = FALSE) +
  geom_text(stat = "stratum", infer.label = TRUE, reverse = FALSE) +
  scale_x_continuous(breaks = 1:3, labels = c("Sex", "Class", "Survived")) +
  ggtitle("Titanic survival by class and sex")

```

---
# Modellierung (1)

`r emo::ji("question")` Was ist ein *Modell*?
> Ein Modell ist ein vereinfachtes Abbild der Wirklichkeit. ([Wikipedia](https://de.wikipedia.org/wiki/Modell))

`r emo::ji("star")` Mit einem Modell versucht man in kompakter Form die Aussage der Daten in anwendbarer Form bereitzustellen.   

* Abstraktion der Realität zu einem bestimmten Zweck. In der Datenanalyse zumeist um Sachverhalte
    * zu verstehen (*deskriptive* Modelle) oder
    * vorherzusagen (*prädiktive* Modelle)

> All models are wrong [, some are useful] ([George Box, 1976](https://en.wikipedia.org/wiki/All_models_are_wrong))

---
# Modellierung (2)

Typische Modelle:
* **Prognose**: Vorhersage zukünftiger Werte von Zeitreihen
* **Segmentierung**: Einteilung von Beobachtungen in Gruppen (*Segmente*), bspw. 
von Kunden in Zielgruppen für Marketingkampagnen
* **Regression**:  Quantifizierung des (statistischen) Zusammenhangs zwischen beobachteten Ausprägungen und einer Zielgrösse 

---
# Beispiel: Daten laden 

[*Telco custumer churn data set* von IBM](https://developer.ibm.com/patterns/data-analysis-model-building-and-deploying-with-wml/)

Der folgende Code lädt die Daten von der Webseite und listet die Spaltennamen:

```{r load_telco_data, message=FALSE}
url <- "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
telco_data <- readr::read_csv(url)
names(telco_data) %>% kable(., format = "html")
```


---
# Beispiel: Übersicht über die Daten

Zufällige Auswahl der ersten 5 Spalten von 10 Datensätzen.

```{r show_telco_data}
telco_data %>% select(one_of(names(.)[1:5])) %>% sample_n(10) %>% 
  kable(., format = "html")
```


# Praktische Datenanalyse

Aufbauend auf dem o.g. Telco-Datensatz führen Sie bitte eine Analyse durch:
* Lesen Sie die Daten ein und prüfen Sie Datenformate und Datentypen
* Wieviele Datensätze sind vollständig: Fehlende Werte?
* Wieviele Datensätze erscheinen sachlich richtig: Ausreisser? Andere Auffälligkeiten?
* Wie sind die Merkmalsausprägungen verteilt: Histogramme, Boxplots?
* 


---
# Qualifikationen für Datenanalyse

Betriebliche Entscheidungen werden zunehmend datenbasiert getroffen
* weil die einfacheren Entscheidungswege ausgereizt sind und
* weil die Verfügbarkeit von Daten und Auswertungsmöglichkeiten es ermöglichen.

`r emo::ji("question")` Was sollte man als Datenanalyst können?

&rarr; Datenbankkenntnisse (spez. relationale DB): *SQL*

&rarr; Besonders in Banken und VU: XML-Daten analysieren: *XMLPath*

&rarr; Muster in Texten verarbeiten: *Reguläre Ausdrücke*

&rarr; Inhalte aus Webseiten extrahieren: *Web scraping*

&rarr; Solide Programmierkenntnisse in *R, Python, VBA, bash, ...*

&rarr; Visualisierungskenntnisse: *Klar, verständlich, richtig*

&rarr; Soft skills: Kritisch sein (auch gg. eigenen Ergebnissen)


---
# Backup

* CAS-Dataset
```{r, eval = FALSE, message = FALSE}
install.packages(c("xts", "sp"))
install.packages("CASdatasets", repos = "http://cas.uqam.ca/pub/R/", type="source")
library(CASdatasets)
data("freMTPL2freq")

```

* Presidential data
```{r, eval = FALSE, message = FALSE}
install.packages("dataMaid")
library(dataMaid)
data("presidentData")

```






