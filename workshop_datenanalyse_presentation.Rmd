---
title: "Workshop Datenanalyse"
subtitle: "mit einem Schwerpunkt auf Versicherungsunternehmen"
author: "Dr.-Ing. Andreas Cardeneo"
institute: "Data Scientist bei Allianz Lebensversicherungs-AG"
email: "andreas.cardeneo@lehre.dhbw-stuttgart.de"
date: "2020-01-13 (Version vom: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"

--- 
class: inverse, center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

library(emo)
library(kableExtra)
library(ggalluvial)
```

# Datenanalyse in Versicherungsunternehmen?

Wir sammeln mal ein paar Aufgaben, die dazu passen.


---
# Der Prozess der Datenanalyse

1. Die richtigen Fragen finden: Was möchte ich eigentlich wissen?
2. Die eigentliche Datenanalyse:

   ![Datenanalyseprozess](./img/data-science-explore.png)

Quelle: [*R for Data Science*](https://r4ds.had.co.nz/explore-intro.html)

Der Punkt *Model* in obiger Graphik bezieht sich auf Datenanalyse im Kontext von *Data Science*, wenn es darum geht, aus den Daten deskriptive oder prädiktive Modelle abzuleiten.

Auf den folgenden Seiten werden die einzelnen Schritte etwas genauer erklärt.


---
# Import

* Dieser Schritt heißt oft auch *Extrakt*, weil es um die Extraktion von Daten aus Quellen geht.
* 

`r emo::ji("question")` Woher beziehen wir eigentlich Daten?


---
# Datenquellen in VU (1)

* Datenbanken - strukturierte Daten
  * OLTP<sup>1</sup>-Systeme z.B. für die Vertragsverwaltung, Zahlvorgänge, Finanzbuchungen
  * OLAP<sup>2</sup>-Systeme für Reporting (Berichtswesen): Nilden die technische Grundlage für  BI<sup>3</sup>-Systeme
  
Meistens werden *relationale Datenbanken* verwendet, auf die mittels `SQL` zugegriffen wird.

.footnote[
[1] *OLTP* - Online Transaction Processing, IT-Systeme, die eine hohe Zahl vergleichsweise kleiner Datenverarbeitungsschritte ausführen, z.B. Datensatzänderungen, Neuanlage von Datensätzen.

[2] *OLAP* - Online Analytical Processing, (interaktive) IT-Systeme für Analysen auf größeren Datenmengen.

[3] *Business Intelligence* - betriebliche Informationssysteme mit Fokus auf vergangenheitsorientiertes Berichtswesen.
]

---
# Datenquellen in VU (2)

* Verwaltungssysteme für unstrukturierte Daten
  * Dokumentenarchive bspw. für Briefe von/an Kunden, Rechtsanwälte, Gutachter, Vertreter usw.
  * Archive für Bilddaten, bspw. Dokumentation von Schadensfällen
  
Diese Verwaltungssysteme lassen sich üblicherweise nicht in vergleichsweise standardisierter Form abfragen wie relationale Datenbanken und die Inhalte müssen erst weiter verarbeitet werden, bspw. mittels OCR<sup>1</sup>. 

.footnote[
[1] *OCR* - Optical Character Recognition, maschinelle Erkennung von (Schrift-)Zeichen, zumeist inkl. Erkennung des Kontexts (Anschrift, Anrede, Betreff, Brieftext usw.).
]

---
# Datenquellen in VU (3)

* Anwendungsdaten
    * insbesondere Office
         * insbesondere `Excel`
  
  Hier liegen die Daten meistens in Form von Dateien in einem Verzeichnis vor. Möchte man die Daten analysieren, müssen diese erst aus den Dokumenten extrahiert oder aus den Anwendungen heraus exportiert werden. Typischer Fall: `CSV`-Dateien für tabellarische Daten.
  
* Technische Protokolle

  Die meisten *Backend*-Systeme wie bspw. *Webserver* protokollieren Zugriffe in Protokolldateien (*Log*-Dateien). Diese Dateien werden normalerweise rollierend überschrieben, bieten aber auf technischer Ebene detaillierte Informationen und insbesondere Zeitstempel. 

  Die Protokolle werden in ähnlicher oder gleicher Form oftmals auch in Datenbanktabellen geschrieben.
        
---
# Datenquellen in VU (4)

* APIs<sup>1</sup>
  
  Modernere IT-Architekturen (heute vielfach [Microservice-Architekturen](https://www.ionos.de/digitalguide/websites/web-entwicklung/microservice-architecture-so-funktionieren-microservices/) definieren technische Schnittstellen, damit die anbietenden Systeme auf relative einfache Weise von außen angesteuert werden können. APIs ermöglichen insbesondere die Abfrage von Zuständen und Daten. Beispiele sind Finanzdaten, Wetterdaten. 

**Beispiel:** Anbieter *Quandl* liefert u.a. Finanzdaten: [https://www.quandl.com/api/v3/datasets/FSE/ALV_X.json?api_key=XXXXXX](https://www.quandl.com/api/v3/datasets/FSE/ALV_X.json)

Für alle oben genannten Datenquellen gilt: Die meisten davon werden überall in Unternehmen genutzt und sind nicht spezifisch für die Versicherungsbranche.

**Was könnte für die Versicherungsbranche besonders sein?**


.footnote[
*API* - Application Programming Interface, technische Schnittstelle zu einem IT-System, um dieses durch andere Programme zu nutzen, d.h. durch die API wird ein IT-System maschinell nutzbar.
]

---
# Besonderheiten von Datenquellen in VU

Versicherungen sind oftmals sehr langlebige Produkte:
* Verträge (bspw. für Lebensversicherungen) laufen über mehrere Jahrzehnte

    `r emo::ji("question")` Wie haben sich IT-Systeme in der Zeit geändert?
    
  &rarr; Wir haben es oftmals mit sehr verschiedenen Technologien zu tun, die *integriert* werden müssen.
  
  &rarr; Betrifft nicht nur Hardware, sondern auch Software z.B. Umlaute in alten Datenbeständen (7 Bit ASCII, kein UTF).
  
  &rarr; Datenstrukturen (bspw. Tabellenschemata) verändern sich über die Zeit wg. wandelnder Anforderungen.


---
# Datenanalyse per Code 

* In vielen Unternehmen wird für die Datenanalyse GUI-basierte Software wie bspw. *PowerBI*, *Tableau*, *Microstrategy*,...<sup>1</sup> verwendet.
* Wird die Datenanalyse per *Code* ausgeführt, also programmmiert, gibt es folgende Vorteile:
    * die *Reproduzierbarkeit* der Ergebnisse ist besser
    * erleichterte *Kollaboration* mittels Versionsverwaltung 
    * weit größere Möglichkeiten der Datentransformation durch *Programmierbarkeit*
    * [Artikel zu Data Science und Programmierung](https://blog.dominodatalab.com/data-scientist-programmer-mutually-exclusive/)


`r emo::ji("backhand index pointing right")` [Crashkurs *Programmieren in R*](https://datentaeter.de/crashkurs-programmieren-in-r-fuer-journalismus-rstudio-coden-datenjournalismus-installation-tutorial/) 

`r emo::ji("backhand index pointing right")` [Datenanalyse in R  1](http://christopherharms.de/stuff/r-workshop/R-Workshop_UniBonn_Tag1.pdf)

`r emo::ji("backhand index pointing right")` [Datenanalyse in R  2](http://christopherharms.de/stuff/r-workshop/R-Workshop_UniBonn_Tag2.pdf)


    
.footnote[
[1] Einige dieser Anwendungen sind auch in gewissem Umfang programmierbar.
]

---
# Nützliche Kenntnisse für die Datenanalyse per Code

* SQL
* XMLPath
* reguläre Ausdrücke
* Web Scraping
* Visualisierung


---
# Datenschutz
	
* *Datenschutzgrundverordnung* (DGSVO) seit 25. Mai 2018 wirksam
* Schutz personenbezogener Daten in der Europäischen Union

`r emo::ji("question")` Was sind *personenbezogene Daten*?

Nach [Art. 4 (1) der DSGVO](https://dejure.org/gesetze/DSGVO/4.html)
> personenbezogene Daten [sind] alle Informationen, die sich auf eine identifizierte oder identifizierbare 
> natürliche Person (im Folgenden "betroffene Person") beziehen; als identifizierbar wird eine natürliche
> Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem
> Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung oder zu einem oder mehreren
> besonderen Merkmalen, die Ausdruck der physischen, physiologischen, genetischen, psychischen, 
> wirtschaftlichen, kulturellen oder sozialen Identität dieser natürlichen Person sind, identifiziert werden
> kann.

---
# Umgang mit personenbezogenen Daten

`r emo::ji("question")` Enthalten die Datenquellen personenbezogene Daten?

`r emo::ji("question")` Werden für die Datenanalyse wirklich personenbezogene Daten (*pbDaten*) benötigt?

* Daten können *anonymisiert* oder *pseudonymisiert* werden:
    * **Anonymisierung**: pbDaten werden unumkehrbar verändert, eine Zuordnung von Merkmalsausprägungen zu Personen ist nicht mehr herstellbar
    * **Pseudonymisierung**: Erklärung nach [Art. 4 DSGVO](https://dsgvo-gesetz.de/art-4-dsgvo/):
    > die Verarbeitung personenbezogener Daten in einer Weise, dass die personenbezogenen Daten ohne Hinzuziehung zusätzlicher Informationen nicht mehr einer spezifischen betroffenen Person zugeordnet werden können, sofern diese zusätzlichen Informationen gesondert aufbewahrt werden und technischen und organisatorischen Maßnahmen unterliegen, die gewährleisten, dass die personenbezogenen Daten nicht einer identifizierten oder identifizierbaren natürlichen Person zugewiesen werden;

---
# Typische Transformationen personenbezogener Daten

* Entfernen von Namen (für Analysezwecke werden diese typischerweise nicht benötigt)
* Geburtsdatum durch Alter ersetzen
* Telefonnummern verkürzen (Vorwahl ausreichend?)
* IBAN: Auf Bankleitzahl verkürzen
* IP-Adresse: Ohnehin nicht aussagekräftig -- Geolokation stattdessen?
* Adresse verkürzen, bspw. auf Postleitzahl
* Analysen mit regionalem Bezug nicht auf kleine Regionen anwenden
 
---
# Qualitätssicherung in der Datenanalyse (1)

* Versionierung

---
# Qualitätssicherung in der Datenanalyse (2)

* data lineage

---
# Qualitätssicherung in der Datenanalyse (3)

* Upload-Timestamp
* Fingerprints


---
# Qualitätssicherung in der Datenanalyse (4)

* technischer Schlüssel


---
# Schritt: Datenquellen lesen

* Datenformat (Trennzeichen, Quoting...)
* Encoding 


---
# Umgang mit sehr großen Datenmengen

* Datenumfang manchmal größer als verfügbare Rechneressourcen (Speicher)
* Einlesen von Teilmengen
    * Deterministisches Einlesen 
    * Zufälliges Einlesen
```{r}
# Beispiel in R
```
    * 
* Verteilte Verarbeitung
    * Erfordert geeignete technische Infrastruktur
        * Cluster von Rechnern
        * Software für die verteilte Speicherung und Verarbeitung
            * Hadoop (HDFS Dateisystem)
            * Spark, Pig, ...


---
# Schritt: Daten verstehen 

* Variablen und Beobachtungen
* Validieren
* Datentypen
* Plotten 
* QS: Magic Numbers

---
# Die allgemeine Struktur von Daten

In den Daten einer Datenanalyse können wir immer wieder die gleiche Struktur erkennen:

* Die Daten bestehen aus einer Menge von gleichartigen *Datensätzen*, auch *Instanzen* oder *Objekte* genannt,
* wobei die einzelnen Datensätze durch bestimmte *Eigenschaften* oder *Merkmale* oder *Variablen* beschrieben werden 
* und diese Merkmale pro Instanz bestimmte *Ausprägungen* haben, die auch *Beobachtungen* genannt werden<sup>1</sup>.

`r emo::ji("backhand index pointing right")` Unternehmensdaten können meistens in Tabellenform extrahiert werden. Die Zeilen entsprechen dann meistens den Instanzen und die Spalten(namen) den Variablen. Die Werte in den Tabellenzellen entsprechen den Beobachtungen. 

.footnote[
[1] Manchmal wird auch die Gesamtheit der Ausprägungen zu einer Instanz Beobachtung genannt. Man spricht dann auch von einem *Datenpunkt* in einem (mehrdimensionalem) Raum.
]


---
# Typen von Variablen

Man unterscheidet zwischen verschiedenen Typen von Variablen:

.pull-left[
  * *Kategoriell*: Die Ausprägungen der Variablen beschreiben *Kategorien*
  * liegen typischerweise als Text vor, bspw. J/N oder M/W 
  * können aber auch Zahlenwerte sein wie 0/1 und als Wahrheitswerte dienen. 
  * Meistens wenig verschiedene Ausprägungen pro Variable.
  * In `R` werden kategorielle Variablen oftmals als *factor* codiert.
  * Nur sinnvoll auf (Un-)Gleichheit prüfbar, kein Rechnen.
]

.pull-right[
  * *Quantitativ*: Die Ausprägungen sind numerische Werte.
  * *Ordinalen* Ausprägungen kann man sinnvoll sortieren aber sie eignen sich nicht zum Rechnen. **Beispiel**: Rangziffern
  * Mit *kardinalen* Ausprägungen kann man rechnen und bspw. Differenzen bilden ([Intervallskala](https://de.wikipedia.org/wiki/Skalenniveau)) oder Verhältnisse ausrechnen ([Verhältnisskala](https://de.wikipedia.org/wiki/Skalenniveau)).
  * Als quantitative Ausprägungen können auch Zeit- und Datumswerte angesehen werden. 
]



---
# Unstrukturierte Daten

Unter *unstrukturierte Daten* versteht man oft Texte, Bilder, Videos oder Audiodaten. 
  * In technischer Hinsicht sind die Daten strukturiert, weil sie in einem definierten Format vorliegen.
  * Für die Analyse ist man aber eher am *Inhalt* der Daten interessiert. 
      * Dann benötigt man Techniken des NLP<sup>1</sup> oder der [Objekterkennung in Bildern](https://medium.com/alegion/deep-learning-for-object-detection-and-localization-using-r-cnn-e88f85ea7c16)):
      
.center[![:scale 30%](./img/bilderobjekte.png)]
      
.footnote[
[1] *NLP*: Natural Language Processing: Methoden zur Verarbeitung natürlicher Sprache. Ziel ist dabei das Textverständnis, um bspw. die Stimmung in Texten zu erkennen (&rarr; [Sentiment Analysis](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html)).
]

---
# Ausreißeranalyse



---
# Fehlende Werte



---
# Besondere Werte


---
# Datenanalyse

* Boxplots, violin plots
* Histogramme
* Scatterplots, 
* Korrelation
* Tests auf statistische Unabhängigkeit
* 

---
 


---
# Visualisierung: Histogramm

Ein *Histogramm* ist ein Häufigkeitsdiagramm. Es dient dazu, die Häufigkeit von Merkmalsausprägungen im Vergleich zu veranschaulichen.

**Beispiel** anhand des `diamonds`-Datensatzes aus `R`:
```{r}
diamonds %>% ggplot() + geom_bar(aes(x = cut))
```


---
# Visualisierung: Boxplot


---
# 

---
# Visualisierung: Alluvial-Diagramme

* ähnlich zu *Parallelkoordinaten* aber für kategorielle Variablen
* `R`-Paket: `ggalluvial`

```{r echo = FALSE}
ggplot(as.data.frame(Titanic),
       aes(y = Freq,
           axis1 = Sex, axis2 = Class, axis3 = Survived)) +
  geom_alluvium(aes(fill = Class),
                width = 0, knot.pos = 0, reverse = FALSE) +
  guides(fill = FALSE) +
  geom_stratum(width = 1/8, reverse = FALSE) +
  geom_text(stat = "stratum", infer.label = TRUE, reverse = FALSE) +
  scale_x_continuous(breaks = 1:3, labels = c("Sex", "Class", "Survived")) +
  ggtitle("Titanic survival by class and sex")

```

---
# Modellierung (1)

`r emo::ji("question")` Was ist ein *Modell*?
> Ein Modell ist ein vereinfachtes Abbild der Wirklichkeit. ([Wikipedia](https://de.wikipedia.org/wiki/Modell))

`r emo::ji("star")` Mit einem Modell versucht man in kompakter Form die Aussage der Daten in anwendbarer Form bereitzustellen.   

* Abstraktion der Realität zu einem bestimmten Zweck. In der Datenanalyse zumeist um Sachverhalte
    * zu verstehen (*deskriptive* Modelle) oder
    * vorherzusagen (*prädiktive* Modelle)

> All models are wrong [, some are useful] ([George Box, 1976](https://en.wikipedia.org/wiki/All_models_are_wrong))

---
# Modellierung (2)

Typische Modelle:
* **Prognose**: Vorhersage zukünftiger Werte von Zeitreihen
* **Segmentierung**: Einteilung von Beobachtungen in Gruppen (*Segmente*), bspw. 
von Kunden in Zielgruppen für Marketingkampagnen
* **Regression**:  Quantifizierung des (statistischen) Zusammenhangs zwischen beobachteten Ausprägungen und einer Zielgrösse 

---
# Beispiel: Daten laden 

[*Telco custumer churn data set* von IBM](https://developer.ibm.com/patterns/data-analysis-model-building-and-deploying-with-wml/)

Der folgende Code lädt die Daten von der Webseite:

```{r load_telco_data, message=FALSE}
library(tidyverse)


url <- "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"

telco_data <- read_csv(url)
```

Liste der Spalten:
```{r}
names(telco_data) %>% kable(., format = "html")
```


---
# Beispiel: Übersicht über die Daten

Zufällige Auswahl der ersten 5 Spalten von 10 Datensätzen.

```{r show_telco_data}
telco_data %>% select(one_of(names(.)[1:5])) %>% sample_n(10) %>% 
  kable(., format = "html")
```

